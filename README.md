# ğŸš€ Automated UML Activity Diagram Generation using LLMs

This research project explores a prompt-based pipeline for automatically generating **UML Activity Diagrams** from natural language requirements using **Large Language Models (LLMs)**.

> ğŸ“… *Minor Research Project â€” M.Tech (Janâ€“May 2025)*  
> ğŸ‘¨â€ğŸ« *Mentor: Saurabh Tiwari*  
> ğŸ§‘â€ğŸ’» *Author: Vrajkumar Patel*

---

## ğŸ“Œ Project Overview

Manually creating UML Activity Diagrams is time-consuming, error-prone, and requires domain expertise. This project presents a **model-agnostic pipeline** that combines:

- **Chain-of-Thought prompting** for structured reasoning
- **ClarifyGPT-style refinement** using an Evaluator LLM
- **PlantUML script generation** for visual output
- **Quantitative and qualitative evaluation** for measuring improvement

---

## ğŸ› ï¸ Tech Stack

- ğŸ§  Large Language Models: Gemini 1.5 Flash, LLaMA 3.2  
- ğŸ§ª Prompt Engineering: Chain-of-Thought (CoT), ClarifyGPT-style evaluation  
- ğŸ–‹ï¸ Scripting & IDEs: Python, Jupyter Notebook, VS Code  
- ğŸ“Š Evaluation: Manual scoring, t-test statistical analysis  
- ğŸ¨ Diagram Rendering: PlantUML  
- ğŸ§ª Experiment Tools: RStudio, Ollama, Git  

---
## ğŸ§­ Methodology

This project combines Chain-of-Thought reasoning and a ClarifyGPT-inspired evaluation loop to iteratively refine UML Activity Diagrams generated from natural language descriptions.

<p align="center">
  <img src="assets/methodology.png" alt="Methodology Diagram" width="700"/>
</p>

---

## ğŸ”„ Workflow

1. **Input**: Natural Language problem statement  
2. **LLM Processing**:
   - Action & control flow extraction (via CoT)
   - Initial diagram generation (PlantUML script)  
3. **Evaluator Loop**:
   - LLaMA 3.2 generates clarifying questions  
   - Responses used to regenerate a refined diagram  
4. **Output**:
   - Final refined diagram with improved correctness and completeness  
5. **Evaluation**:
   - Scored across 5 dimensions: Completeness, Correctness, Standards, Terminology, Understandability

---

## ğŸ“ˆ Results Summary

The ClarifyGPT-enhanced pipeline showed consistent improvements in diagram quality across three real-world problem statements.

<p align="center">
  <img src="assets/grouped_metrics_bar_chart.png" alt="Evaluation Results Bar Graph" width="700"/>
</p>

| Metric              | Before (Mean) | After (Mean) | % Improvement | Significance |
|---------------------|----------------|---------------|----------------|---------------|
| Completeness        | 2.2            | 3.6           | +63.6%         | âœ… Significant |
| Correctness         | 3.0            | 4.0           | +33.3%         | âœ… Significant |
| Standards Adherence | 4.6            | 5.0           | +8.7%          | âŒ Not Sig.    |
| Understandability   | 3.6            | 4.2           | +16.7%         | âŒ Not Sig.    |
| Terminology         | 4.8            | 5.0           | +4.2%          | âŒ Not Sig.    |

---
## ğŸ“„ Resources

- ğŸ“‘ **Final Report:** [docs/final_report.pdf](docs/final_report.pdf)
- ğŸ¤ **Presentation Slides:** [docs/presentation_slides.pdf](docs/presentation_slides.pdf)

---

## ğŸ” Research Questions

- **RQ1:** What is the quality of Activity Diagrams generated by LLMs from natural language input?
- **RQ2:** What common modeling issues occur, and how can clarifying questions resolve them?

---

## ğŸ“œ References

- ğŸ“„ *ClarifyGPT: A framework for enhancing LLM-based code generation via requirements clarification*
- ğŸ“„ *Model Generation with LLMs: From Requirements to UML Diagrams*
- ğŸ“˜ [PlantUML Official Documentation](https://plantuml.com/activity-diagram)

---

## ğŸ¤ Contributions

This research was independently conducted as part of the **M.Tech Research Project** (Janâ€“May 2025).  
Pull requests and suggestions to extend this work to other UML diagram types (e.g., Class Diagrams, State Machines) are welcome!

---

## ğŸ“¬ Contact

For queries or collaboration opportunities:

ğŸ“§ **31vrajpatel@gmail.com**

---
