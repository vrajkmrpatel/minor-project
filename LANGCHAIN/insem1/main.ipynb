{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART - B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USED LLMs\n",
    "####   llama3.2 | deepseek-r1:1.5b | mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: langchain-ollama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-ollama) (0.3.33)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.3.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2023.7.22)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.10.15)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.2.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# using ollama\n",
    "%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4288\\2112080848.py:7: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(model=model_name, temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Define function to initialize different local LLMs\n",
    "def get_local_llm(model_name):\n",
    "    return ChatOllama(model=model_name, temperature=0.7)\n",
    "\n",
    "# Available local models\n",
    "llama_model = get_local_llm(\"llama3.2\")\n",
    "deepseek_model = get_local_llm(\"deepseek-r1:1.5b\")\n",
    "mistral_model = get_local_llm(\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement=\"\"\"NYU, an insurance company wants to digitize a range of business processes and provide a complete solution that addresses all aspects of the agent-insurer relationship. Consider yourself as a part of Requirement Analyst team at Retinodes Software Company, and your job is to gather and prioritize the set of requirements. In this new requirement of the project, there are no existing systems that can be analyzed for the development. Requirements have to be gathered, negotiated, validated and prioritized through multiple stakeholders which is a complex process because all stakeholders have different perspectives, requirements and priorities. Therefore, Retinodes want to have a requirements engineering framework that can be used in market-facing projects. To start with, you need to identify the set of stakeholders associated with the system, the domain information about the insurance market, and possible features. The first product NYU wanted you to develop consolidated insurance packages which can compete with the packages provided by other insurance companies. Another product is based on the customer priority, based on the insurance policies available the customer can create his/her own package and send a request for the review. The system has to automatically analyze the package, provide suggestions (if any), and at last give a competing price for the package. To understand the problem domain, existing packages has to be analyzed and the demands and restrictions from the insurance policy and agents have to be understood completely. The requirements and feasibility report generated by you, will further used by the development team for implementation.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Stakeholders, End-Users, Functional & Non-functional Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(problem_statement):\n",
    "    return f\"\"\"\n",
    "### Role Assignment:\n",
    "You are a **requirement analyst** responsible for gathering and analyzing system requirements for a **Market-Driven Insurance System**. Your job is to identify stakeholders, end-users, and system requirements based on the given problem statement.\n",
    "\n",
    "### Task:\n",
    "Analyze the problem statement and extract the following:\n",
    "1Ô∏è‚É£ **Stakeholders:** Identify **primary** and **secondary** stakeholders.\n",
    "2Ô∏è‚É£ **End-users:** List the different categories of end-users.\n",
    "3Ô∏è‚É£ **Functional Requirements:** Identify system features and functionalities.\n",
    "4Ô∏è‚É£ **Non-functional Requirements:** Identify system constraints, performance, security, and usability aspects.\n",
    "\n",
    "### Contextual Information:\n",
    "- **Primary stakeholders** are directly involved in system development, usage, or decision-making.\n",
    "- **Secondary stakeholders** are indirectly affected but influence the system.\n",
    "- **Functional requirements** define the features the system **must perform**.\n",
    "- **Non-functional requirements** define **how** the system should behave (performance, security, scalability, etc.).\n",
    "\n",
    "### Instructions:\n",
    "- **Use Few-Shot Prompting:** Refer to the following example to ensure accurate extraction:\n",
    "  **Example:**\n",
    "  üîπ **Primary Stakeholders:** Insurance Company Executives, Business Analysts\n",
    "  üîπ **Secondary Stakeholders:** Government Regulators, Competing Insurance Firms\n",
    "  üîπ **End-users:** Insurance Agents, Customers\n",
    "  üîπ **Functional Requirement:** The system should allow customers to create personalized insurance packages.\n",
    "  üîπ **Non-functional Requirement:** The system should support **high availability** with 99.9% uptime.\n",
    "\n",
    "- **Output format:** Provide structured output in a numbered list.\n",
    "\n",
    "### Problem Statement:\n",
    "{problem_statement}\n",
    "\n",
    "### Closing:\n",
    "If any information is ambiguous or unclear, make logical assumptions and document them explicitly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Output saved to mistral_output.txt\n"
     ]
    }
   ],
   "source": [
    "def run_llm(model, prompt):\n",
    "    response = model([SystemMessage(content=\"You are an expert requirement analyst.\"), \n",
    "                      HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function to save output to a text file\n",
    "def save_output(model_name, output):\n",
    "    filename = f\"{model_name}_output.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"Model: {model_name}\\n\")\n",
    "        file.write(\"==================================\\n\")\n",
    "        file.write(output + \"\\n\")\n",
    "    print(f\"‚úÖ Output saved to {filename}\")\n",
    "    \n",
    "# Example Prompt\n",
    "prompt = generate_prompt(problem_statement)\n",
    "\n",
    "# Run models and save results\n",
    "# llama_output = run_llm(llama_model, prompt)\n",
    "# save_output(\"llama3.2\", llama_output)\n",
    "\n",
    "# deepseek_output = run_llm(deepseek_model, prompt)\n",
    "# save_output(\"deepseek-r1\", deepseek_output)\n",
    "\n",
    "mistral_output = run_llm(mistral_model, prompt)\n",
    "save_output(\"mistral\", mistral_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gnerate User Stories from Problem Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running LLM: mistral...\n",
      "‚úÖ User stories saved successfully in 'mistral_user_stories_output.txt'!\n",
      "\n",
      "üéâ All models have completed execution!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Function to generate user stories\n",
    "def generate_user_stories(model_name, problem_statement, step1_output):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert requirement analyst specializing in user stories following the INVEST framework.\n",
    "\n",
    "    ## Problem Statement:\n",
    "    {problem_statement}\n",
    "\n",
    "    ## Context Specification (Output from Step 1):\n",
    "    {step1_output}\n",
    "\n",
    "    ## Guidelines:\n",
    "    - Generate user stories that are **Independent, Negotiable, Valuable, Estimable, Small, and Testable** (INVEST).\n",
    "    - Use the format:\n",
    "        **Front of the Card:**  \n",
    "        *As a [role], I want to [action] so that [benefit].*  \n",
    "        **Back of the Card:**  \n",
    "        - **Success:** \n",
    "            (a) [List conditions for success]  \n",
    "            (b) [List conditions for success]  \n",
    "        - **Failure - Display Message:** \n",
    "            (a) [List error scenarios]\n",
    "            (b) [List error scenarios]\n",
    "\n",
    "    ## Few-Shot Examples:\n",
    "\n",
    "    **Front of the Card:**  \n",
    "    As a customer, I want to browse and compare insurance packages so that I can choose the best plan for my needs.  \n",
    "\n",
    "    **Back of the Card:**  \n",
    "    - **Success:**  \n",
    "        (a) Display available insurance packages from different providers.  \n",
    "        (b) Show comparison table with features, pricing, and benefits.  \n",
    "    - **Failure - Display Message:**  \n",
    "        (a) \"No available insurance packages at the moment, please check later.\"  \n",
    "        (b) \"System error, please try again.\"\n",
    "\n",
    "    **Front of the Card:**  \n",
    "    As an insurance agent, I want to manage customer policies so that I can assist clients with their insurance needs.  \n",
    "\n",
    "    **Back of the Card:**  \n",
    "    - **Success:**  \n",
    "        (a) View customer policies and their details.  \n",
    "        (b) Update policy information upon customer request.  \n",
    "        (c) Generate a new quote based on customer requirements.  \n",
    "    - **Failure - Display Message:**  \n",
    "        (a) \"Customer policy not found.\"  \n",
    "        (b) \"Unauthorized access, please log in as an agent.\"  \n",
    "        (c) \"System error, please try again later.\"  \n",
    "\n",
    "    ## Task:\n",
    "    Based on the problem statement and step1_output files,\n",
    "    generate 10 user stories following the INVEST format.\n",
    "    Ensure stories **cover different Stakeholders(Primary & Secondary) and End-users** and **include success and failure cases**.\n",
    "\n",
    "    Output **ONLY** the user stories in the specified format.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    response = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Define the list of models\n",
    "# models = [\"llama3.2\", \"deepseek-r1:1.5b\", \"mistral\"]\n",
    "models = [\"mistral\"]\n",
    "\n",
    "# Function to read content from a file\n",
    "def read_from_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read().strip()\n",
    "    \n",
    "# Loop through each model and generate user stories\n",
    "for model_name in models:\n",
    "    print(f\"üöÄ Running LLM: {model_name}...\")\n",
    "\n",
    "    # Handle deepseek-r1:1.5b separately for file naming\n",
    "    if model_name == \"deepseek-r1:1.5b\":\n",
    "        step1_filename = \"deepseek-r1_output.txt\"\n",
    "        output_filename = \"deepseek-r1_user_stories_output.txt\"\n",
    "    else:\n",
    "        step1_filename = f\"{model_name}_output.txt\"\n",
    "        output_filename = f\"{model_name}_user_stories_output.txt\"\n",
    "\n",
    "    # Read Step 1 Output for the respective model\n",
    "    step1_output = read_from_file(step1_filename)\n",
    "\n",
    "    # Generate user stories\n",
    "    user_stories_output = generate_user_stories(model_name, problem_statement, step1_output)\n",
    "\n",
    "    # Save Output to a .txt File\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(user_stories_output)\n",
    "\n",
    "    print(f\"‚úÖ User stories saved successfully in '{output_filename}'!\\n\")\n",
    "\n",
    "print(\"üéâ All models have completed execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Identify 3 different EPICs (or collection of user stories) where the conflicts between the requirements occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running LLM: mistral...\n",
      "‚úÖ Conflicting EPICs saved successfully in 'mistral_conflicting_epics.txt'!\n",
      "\n",
      "üéâ All models have completed execution!\n"
     ]
    }
   ],
   "source": [
    "# Function to generate conflicting EPICs\n",
    "def generate_conflicting_epics(model_name, problem_statement, user_stories):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert business analyst specializing in requirement conflict resolution.\n",
    "\n",
    "    ## Problem Statement:\n",
    "    {problem_statement}\n",
    "\n",
    "    ## User Stories:\n",
    "    {user_stories}\n",
    "\n",
    "    ## Task:\n",
    "    - Identify **three EPICs** where **conflicts between requirements occur**.\n",
    "    - Each EPIC should include **two conflicting user stories**.\n",
    "    - Clearly explain the **conflict**.\n",
    "    - Propose a **resolution strategy**.\n",
    "\n",
    "    ## Example EPIC with Conflict:\n",
    "    \n",
    "    **EPIC: Policy Customization vs. Standardized Offerings**  \n",
    "    **Conflicting User Stories:**  \n",
    "    - As a customer, I want to fully customize my insurance package so that I can get exactly what I need.  \n",
    "    - As NYU Management, I want to offer only a few well-defined insurance packages so that our operations remain scalable and easy to manage.  \n",
    "      \n",
    "    **Conflict:**  \n",
    "    - Customers want flexibility, but NYU Management prefers structured, pre-defined policies to reduce complexity & risks.  \n",
    "      \n",
    "    **Resolution Strategy:**  \n",
    "    ‚úÖ **Guided Customization:**  \n",
    "      - Offer pre-configured packages as a starting point.  \n",
    "      - Allow limited customization within predefined parameters (e.g., adjusting coverage amount, adding riders).  \n",
    "      - Use AI-driven recommendations to help users choose the best package without making the process overly complex.  \n",
    "\n",
    "    **Now, generate three EPICs with similar conflict-resolution analysis.**\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Define the list of models\n",
    "# models = [\"llama3.2\", \"deepseek-r1:1.5b\", \"mistral\"]\n",
    "models = [\"mistral\"]\n",
    "\n",
    "# Function to read content from a file\n",
    "def read_from_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read().strip()\n",
    "    \n",
    "# Loop through each model and generate user stories\n",
    "for model_name in models:\n",
    "    print(f\"üöÄ Running LLM: {model_name}...\")\n",
    "\n",
    "    # Handle deepseek-r1:1.5b separately for file naming\n",
    "    if model_name == \"deepseek-r1:1.5b\":\n",
    "        user_stories_filename = \"deepseek-r1_user_stories_output.txt\"\n",
    "        epics_output_filename = \"deepseek-r1_conflicting_epics.txt\"\n",
    "    else:\n",
    "        user_stories_filename = f\"{model_name}_user_stories_output.txt\"\n",
    "        epics_output_filename = f\"{model_name}_conflicting_epics.txt\"\n",
    "\n",
    "    # Read Step 1 Output for the respective model\n",
    "    user_stories = read_from_file(user_stories_filename)\n",
    "\n",
    "    # Generate user stories\n",
    "    conflicting_epics_output = generate_conflicting_epics(model_name, problem_statement, user_stories)\n",
    "\n",
    "    # Save Output to a .txt File\n",
    "    with open(epics_output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(conflicting_epics_output)\n",
    "\n",
    "    print(f\"‚úÖ Conflicting EPICs saved successfully in '{epics_output_filename}'!\\n\")\n",
    "\n",
    "print(\"üéâ All models have completed execution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
