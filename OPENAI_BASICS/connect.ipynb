{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a simple request to OpenAI\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are my co-requirement analyst.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How we can extract goals and actors from the given problem in requirements.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ollama model = llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ollama\n",
    "%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To answer the question \"What is LangChain?\", let\\'s break it down step by step:\\n\\n1. **Understanding the Name**: The name \"LangChain\" might seem like a mix of \"language\" and \"chain\", which could hint at its connection to natural language processing or data storage.\\n\\n2. **Researching Online**: Upon searching online, we find that LangChain is an open-source framework primarily used for building decentralized web applications, especially those related to AI and machine learning. It offers a set of tools designed to simplify the development process by providing an interface between blockchain and machine learning models.\\n\\n3. **Blockchain and Machine Learning Integration**: The key aspect of LangChain lies in its ability to integrate blockchain technology with AI models. This integration enables the creation of decentralized applications that can store, manage, and execute AI-driven tasks across various platforms and networks.\\n\\n4. **Key Features and Use Cases**: Some notable features of LangChain include its support for multiple blockchain protocols (such as Ethereum), its interoperability capabilities between different blockchains, and its ability to integrate with various machine learning frameworks. This versatility makes it an attractive solution for developers seeking to build decentralized applications that leverage AI.\\n\\n5. **Community Involvement and Development**: The community behind LangChain actively contributes to its development, providing support, documentation, and new features through open-source development channels.\\n\\nBy examining these steps, we can gain a comprehensive understanding of what LangChain is: an innovative framework designed to bridge the gap between blockchain technology and machine learning models, facilitating the creation of decentralized AI-powered applications.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wATrUaQJjCTUQuV2iCGEZnro5Syc7vqqvkEsMdcf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cohere_api = os.getenv(\"CO_API_KEY\")\n",
    "cohere_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# from langchain_cohere.llms import Cohere\n",
    "\n",
    "# llm = Cohere()\n",
    "# print(llm.invoke(\"Come up with a pet name\"))\n",
    "\n",
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello World!\"}],\n",
    ")\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDAUO0Ib2tP0EaEzk-0cUtCNRtXaT6svXU'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GEMINI_API = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In the realm of code, where logic flows,\n",
      "Let's sing a Python song, as our rhythm grows.\n",
      "With indents deep and variables galore,\n",
      "We'll create melodies that we'll forever adore.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Python, Python, our language so grand,\n",
      "You make coding a breeze, with a helping hand.\n",
      "From data science to web design,\n",
      "You're the tool we need, to make our dreams align.\n",
      "\n",
      "(Verse 2)\n",
      "With lists and tuples, we'll store our data,\n",
      "Dictionaries hold keys, and none is our mantra.\n",
      "Functions and classes, oh how they shine,\n",
      "Encapsulating code, making it divine.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Python, Python, our language so grand,\n",
      "You make coding a breeze, with a helping hand.\n",
      "From data science to web design,\n",
      "You're the tool we need, to make our dreams align.\n",
      "\n",
      "(Bridge)\n",
      "From loops to comprehensions, we've got it all,\n",
      "Python's power, we'll never let it fall.\n",
      "With modules and packages, we'll extend our reach,\n",
      "Building applications that will fulfill our speech.\n",
      "\n",
      "(Verse 3)\n",
      "With algorithms and data structures in sight,\n",
      "We'll solve problems complex, day and night.\n",
      "Object-oriented programming, a concept so true,\n",
      "Encapsulation, inheritance, and polymorphism, we'll pursue.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Python, Python, our language so grand,\n",
      "You make coding a breeze, with a helping hand.\n",
      "From data science to web design,\n",
      "You're the tool we need, to make our dreams align.\n",
      "\n",
      "(Outro)\n",
      "So let's raise our voices, and sing this song,\n",
      "To Python, the language, where we truly belong.\n",
      "May its power inspire, and its elegance guide,\n",
      "As we journey together, side by side.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GEMINI_API)\n",
    "response = llm.invoke(\"give me python song that i can sing with u\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ollama model = deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I note that there were initially 3 cars in the parking lot.\n",
      "\n",
      "Next, 2 additional cars arrive and park somewhere else.\n",
      "\n",
      "To find the total number of cars in the parking lot after this change, I need to add the initial number of cars to the number of cars that arrived.\n",
      "\n",
      "So, 3 (initial) + 2 (arrived) = 5 (total).\n",
      "\n",
      "Therefore, there are now 5 cars in the parking lot.\n",
      "</think>\n",
      "\n",
      "Sure, let's solve the problem step by step:\n",
      "\n",
      "**Problem Statement:**\n",
      "If there are **3 cars** in the parking lot and **2 more cars arrive**, how many cars are in the parking lot?\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "1. **Initial Number of Cars:**\n",
      "   - There are **3 cars** initially in the parking lot.\n",
      "\n",
      "2. **Number of Cars Arriving:**\n",
      "   - **2 more cars** come into the parking lot and park somewhere else.\n",
      "\n",
      "3. **Calculating the Total Number of Cars:**\n",
      "   - To find the total number of cars after the new arrivals, we add the number of arriving cars to the initial count.\n",
      "   \n",
      "   \\[\n",
      "   \\text{Total Cars} = \\text{Initial Cars} + \\text{Arriving Cars}\n",
      "   \\]\n",
      "   \n",
      "   \\[\n",
      "   \\text{Total Cars} = 3 + 2 = 5\n",
      "   \\]\n",
      "\n",
      "4. **Final Answer:**\n",
      "   \n",
      "   There are \\(\\boxed{5}\\) cars in the parking lot now.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "question=\"\"\"\n",
    "    Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, therewill be 21 trees. How many trees did the grove workers plant today?\n",
    "    A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have\n",
    "    been 21 - 15 = 6. The answer is 6.\n",
    "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response=chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
